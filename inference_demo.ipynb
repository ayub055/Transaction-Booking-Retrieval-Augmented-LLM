{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transaction Tagging Inference Demo\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Load the trained model and golden record index\n",
    "2. Run batch predictions on a CSV file\n",
    "3. Display predicted labels and retrieved similar transactions\n",
    "4. Identify potential mislabeled golden records\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Ensure you have:\n",
    "- ‚úÖ Trained model: `experiments/.../fusion_encoder_best.pth`\n",
    "- ‚úÖ Training artifacts: `training_artifacts/training_artifacts.pkl`\n",
    "- ‚úÖ FAISS index: `golden_records.faiss`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.inference_pipeline import TransactionInferencePipeline, print_prediction_result\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Pipeline\n",
    "\n",
    "Load the trained model and golden record index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "ARTIFACTS_PATH = \"training_artifacts/training_artifacts.pkl\"\n",
    "MODEL_PATH = \"experiments/tagger_proj256_final256_freeze-gradual_bs2048_lr5.66e-05/fusion_encoder_best.pth\"\n",
    "INDEX_PATH = \"golden_records.faiss\"\n",
    "TOP_K = 5  # Number of similar transactions to retrieve\n",
    "\n",
    "# Initialize pipeline\n",
    "pipeline = TransactionInferencePipeline(\n",
    "    artifacts_path=ARTIFACTS_PATH,\n",
    "    model_path=MODEL_PATH,\n",
    "    index_path=INDEX_PATH\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Pipeline initialized successfully!\")\n",
    "print(f\"   Index size: {pipeline.index.ntotal} golden records\")\n",
    "print(f\"   Categories: {len(pipeline.label_mapping)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Your Test CSV\n",
    "\n",
    "**Upload your CSV file** or specify the path below.\n",
    "\n",
    "Required columns:\n",
    "- `tran_partclr` (transaction description)\n",
    "- `tran_mode` (transaction mode)\n",
    "- `dr_cr_indctor` (debit/credit indicator)\n",
    "- `sal_flag` (salary flag)\n",
    "- `tran_amt_in_ac` (transaction amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your CSV file\n",
    "CSV_PATH = \"data/sample_txn.csv\"  # ‚Üê Change this to your test file\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(df)} transactions from {CSV_PATH}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nFirst 3 rows:\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Batch Prediction\n",
    "\n",
    "Predict labels for all transactions in the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to list of dictionaries\n",
    "transactions = df.to_dict('records')\n",
    "\n",
    "# Run batch prediction\n",
    "print(f\"Running predictions on {len(transactions)} transactions...\\n\")\n",
    "\n",
    "results = pipeline.predict_batch(transactions, top_k=TOP_K)\n",
    "\n",
    "print(f\"‚úÖ Batch prediction complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. View Results Summary\n",
    "\n",
    "Quick overview of all predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "\n",
    "for i, (txn, result) in enumerate(zip(transactions, results)):\n",
    "    summary_data.append({\n",
    "        'Index': i,\n",
    "        'Description': txn['tran_partclr'][:50] + '...' if len(txn['tran_partclr']) > 50 else txn['tran_partclr'],\n",
    "        'Amount': txn['tran_amt_in_ac'],\n",
    "        'Predicted_Category': result['predicted_category'],\n",
    "        'Confidence': f\"{result['confidence']:.1%}\",\n",
    "        'Votes': f\"{int(result['confidence'] * TOP_K)}/{TOP_K}\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PREDICTION SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Detailed View: Individual Predictions\n",
    "\n",
    "Examine each prediction with retrieved similar transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which transaction to inspect (change this number)\n",
    "TRANSACTION_INDEX = 0  # ‚Üê Change this to view different transactions\n",
    "\n",
    "txn = transactions[TRANSACTION_INDEX]\n",
    "result = results[TRANSACTION_INDEX]\n",
    "\n",
    "# Print detailed result\n",
    "print_prediction_result(result, txn, TOP_K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Interactive Viewer\n",
    "\n",
    "Loop through all transactions and display results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all transactions with details\n",
    "NUM_TO_DISPLAY = 10  # ‚Üê Change to see more/fewer\n",
    "\n",
    "for i in range(min(NUM_TO_DISPLAY, len(transactions))):\n",
    "    txn = transactions[i]\n",
    "    result = results[i]\n",
    "    \n",
    "    print(f\"\\n{'#'*80}\")\n",
    "    print(f\"TRANSACTION {i+1}/{len(transactions)}\")\n",
    "    print(f\"{'#'*80}\")\n",
    "    \n",
    "    print(f\"\\nQuery:\")\n",
    "    print(f\"  Description: {txn['tran_partclr']}\")\n",
    "    print(f\"  Amount: ${txn['tran_amt_in_ac']:.2f}\")\n",
    "    print(f\"  Mode: {txn['tran_mode']} | DR/CR: {txn['dr_cr_indctor']}\")\n",
    "    \n",
    "    print(f\"\\nPrediction:\")\n",
    "    print(f\"  Category: {result['predicted_category']}\")\n",
    "    print(f\"  Confidence: {result['confidence']:.2%} ({int(result['confidence']*TOP_K)}/{TOP_K} votes)\")\n",
    "    \n",
    "    print(f\"\\nVote Distribution:\")\n",
    "    for category, count in sorted(result['vote_distribution'].items(), key=lambda x: x[1], reverse=True):\n",
    "        print(f\"  {category}: {count} vote(s)\")\n",
    "    \n",
    "    print(f\"\\nRetrieved Similar Transactions:\")\n",
    "    for j, similar in enumerate(result['similar_transactions'], 1):\n",
    "        similar_txn = similar['transaction']\n",
    "        print(f\"\\n  {j}. Golden Record Index: {similar['index']}\")\n",
    "        print(f\"     Label: {similar['label']}\")\n",
    "        print(f\"     Description: {similar_txn['description']}\")\n",
    "        print(f\"     Amount: ${similar_txn['amount']:.2f}\")\n",
    "        print(f\"     Similarity Distance: {similar['similarity_distance']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n{'-'*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Identify Low Confidence Predictions\n",
    "\n",
    "Find transactions that may need manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set confidence threshold\n",
    "CONFIDENCE_THRESHOLD = 0.6  # 60%\n",
    "\n",
    "low_confidence = []\n",
    "\n",
    "for i, (txn, result) in enumerate(zip(transactions, results)):\n",
    "    if result['confidence'] < CONFIDENCE_THRESHOLD:\n",
    "        low_confidence.append({\n",
    "            'Index': i,\n",
    "            'Description': txn['tran_partclr'],\n",
    "            'Predicted': result['predicted_category'],\n",
    "            'Confidence': result['confidence'],\n",
    "            'Votes': result['vote_distribution']\n",
    "        })\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è Found {len(low_confidence)} low-confidence predictions (< {CONFIDENCE_THRESHOLD:.0%})\\n\")\n",
    "\n",
    "if low_confidence:\n",
    "    print(\"=\"*80)\n",
    "    print(\"LOW CONFIDENCE PREDICTIONS - REVIEW RECOMMENDED\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for item in low_confidence[:10]:  # Show first 10\n",
    "        print(f\"\\nIndex {item['Index']}:\")\n",
    "        print(f\"  Description: {item['Description'][:60]}...\")\n",
    "        print(f\"  Predicted: {item['Predicted']} (Confidence: {item['Confidence']:.1%})\")\n",
    "        print(f\"  Vote breakdown: {item['Votes']}\")\n",
    "else:\n",
    "    print(\"‚úÖ All predictions have high confidence!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Check for Potential Mislabeled Golden Records\n",
    "\n",
    "If you have ground truth labels in your CSV, compare them with retrieved golden records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If your CSV has a 'category' column with ground truth\n",
    "if 'category' in df.columns:\n",
    "    suspicious_golden_records = []\n",
    "    \n",
    "    for i, (txn, result) in enumerate(zip(transactions, results)):\n",
    "        expected_label = txn['category']\n",
    "        predicted_label = result['predicted_category']\n",
    "        \n",
    "        # If prediction is wrong\n",
    "        if expected_label != predicted_label:\n",
    "            # Check if any retrieved golden records have wrong labels\n",
    "            for similar in result['similar_transactions']:\n",
    "                if similar['label'] != expected_label:\n",
    "                    suspicious_golden_records.append({\n",
    "                        'golden_index': similar['index'],\n",
    "                        'golden_description': similar['transaction']['description'],\n",
    "                        'golden_label': similar['label'],\n",
    "                        'query_description': txn['tran_partclr'],\n",
    "                        'expected_label': expected_label,\n",
    "                        'distance': similar['similarity_distance']\n",
    "                    })\n",
    "    \n",
    "    print(f\"\\nüîç Found {len(suspicious_golden_records)} potentially mislabeled golden records\\n\")\n",
    "    \n",
    "    if suspicious_golden_records:\n",
    "        # Remove duplicates by golden_index\n",
    "        unique_suspicious = {item['golden_index']: item for item in suspicious_golden_records}\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"POTENTIALLY MISLABELED GOLDEN RECORDS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for golden_idx, item in list(unique_suspicious.items())[:10]:\n",
    "            print(f\"\\nGolden Record Index: {item['golden_index']}\")\n",
    "            print(f\"  Description: {item['golden_description']}\")\n",
    "            print(f\"  Current Label: {item['golden_label']}\")\n",
    "            print(f\"  Similar to: {item['query_description'][:50]}...\")\n",
    "            print(f\"  Expected Label: {item['expected_label']}\")\n",
    "            print(f\"  Distance: {item['distance']:.4f}\")\n",
    "            print(f\"  ‚ö†Ô∏è Consider reviewing this label!\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è No ground truth 'category' column found in CSV\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results to CSV\n",
    "\n",
    "Save predictions for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare export data\n",
    "export_data = []\n",
    "\n",
    "for i, (txn, result) in enumerate(zip(transactions, results)):\n",
    "    # Base record\n",
    "    record = {\n",
    "        'transaction_index': i,\n",
    "        'description': txn['tran_partclr'],\n",
    "        'amount': txn['tran_amt_in_ac'],\n",
    "        'mode': txn['tran_mode'],\n",
    "        'dr_cr': txn['dr_cr_indctor'],\n",
    "        'predicted_category': result['predicted_category'],\n",
    "        'confidence': result['confidence'],\n",
    "        'vote_distribution': str(result['vote_distribution']),\n",
    "    }\n",
    "    \n",
    "    # Add ground truth if available\n",
    "    if 'category' in txn:\n",
    "        record['ground_truth'] = txn['category']\n",
    "        record['correct'] = txn['category'] == result['predicted_category']\n",
    "    \n",
    "    # Add top-3 similar transactions\n",
    "    for j, similar in enumerate(result['similar_transactions'][:3], 1):\n",
    "        record[f'similar_{j}_index'] = similar['index']\n",
    "        record[f'similar_{j}_description'] = similar['transaction']['description']\n",
    "        record[f'similar_{j}_label'] = similar['label']\n",
    "        record[f'similar_{j}_distance'] = similar['similarity_distance']\n",
    "    \n",
    "    export_data.append(record)\n",
    "\n",
    "# Create DataFrame and export\n",
    "export_df = pd.DataFrame(export_data)\n",
    "output_path = 'inference_results.csv'\n",
    "export_df.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Results exported to: {output_path}\")\n",
    "print(f\"   Total predictions: {len(export_df)}\")\n",
    "print(f\"\\nPreview:\")\n",
    "export_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Statistics & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"INFERENCE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Confidence distribution\n",
    "confidences = [r['confidence'] for r in results]\n",
    "print(f\"\\nConfidence Scores:\")\n",
    "print(f\"  Mean: {np.mean(confidences):.2%}\")\n",
    "print(f\"  Median: {np.median(confidences):.2%}\")\n",
    "print(f\"  Min: {np.min(confidences):.2%}\")\n",
    "print(f\"  Max: {np.max(confidences):.2%}\")\n",
    "\n",
    "# Category distribution\n",
    "predicted_categories = [r['predicted_category'] for r in results]\n",
    "category_counts = pd.Series(predicted_categories).value_counts()\n",
    "print(f\"\\nPredicted Category Distribution:\")\n",
    "for category, count in category_counts.items():\n",
    "    print(f\"  {category}: {count} ({count/len(results):.1%})\")\n",
    "\n",
    "# Accuracy (if ground truth available)\n",
    "if 'category' in df.columns:\n",
    "    correct = sum(1 for txn, result in zip(transactions, results) \n",
    "                  if txn['category'] == result['predicted_category'])\n",
    "    accuracy = correct / len(results)\n",
    "    print(f\"\\nAccuracy: {accuracy:.2%} ({correct}/{len(results)})\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Visualizations (Optional)\n",
    "\n",
    "Uncomment if you want plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# # Confidence distribution\n",
    "# plt.figure(figsize=(10, 5))\n",
    "# plt.hist([r['confidence'] for r in results], bins=20, edgecolor='black')\n",
    "# plt.xlabel('Confidence')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Confidence Distribution')\n",
    "# plt.axvline(x=0.6, color='r', linestyle='--', label='Threshold (60%)')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Category distribution\n",
    "# plt.figure(figsize=(12, 6))\n",
    "# category_counts.plot(kind='bar')\n",
    "# plt.xlabel('Category')\n",
    "# plt.ylabel('Count')\n",
    "# plt.title('Predicted Category Distribution')\n",
    "# plt.xticks(rotation=45, ha='right')\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "‚úÖ **What you learned:**\n",
    "1. How to load the inference pipeline\n",
    "2. How to run batch predictions on a CSV file\n",
    "3. How to view predicted labels and similar transactions\n",
    "4. How to identify low-confidence predictions\n",
    "5. How to spot potentially mislabeled golden records\n",
    "6. How to export results for further analysis\n",
    "\n",
    "**Next Steps:**\n",
    "- Review low-confidence predictions manually\n",
    "- Investigate potentially mislabeled golden records\n",
    "- Add corrected labels to your dataset\n",
    "- Rebuild golden record index with `run_inference.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
